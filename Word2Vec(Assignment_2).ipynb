{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zfF8S1UhtU2o"
      },
      "outputs": [],
      "source": [
        "# sample data\n",
        "documents = [\n",
        "    \"I love natural language processing\",\n",
        "    \"I love machine learning\",\n",
        "    \"Machine learning loves data\"\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BoW (Count)"
      ],
      "metadata": {
        "id": "AYRFRQrPurIG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "cv = CountVectorizer()\n",
        "bow = cv.fit_transform(documents)\n",
        "\n",
        "print(cv.get_feature_names_out())\n",
        "print(bow.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jHZ0GrDFuuC1",
        "outputId": "de36775c-8551-4969-a27c-5a795b3566fb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['data' 'language' 'learning' 'love' 'loves' 'machine' 'natural'\n",
            " 'processing']\n",
            "[[0 1 0 1 0 0 1 1]\n",
            " [0 0 1 1 0 1 0 0]\n",
            " [1 0 1 0 1 1 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalized BoW"
      ],
      "metadata": {
        "id": "KOhXm_cVvHQU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.preprocessing import normalize\n",
        "\n",
        "bow_normalized = normalize(bow, norm='l1')\n",
        "\n",
        "print(cv.get_feature_names_out())\n",
        "print(bow_normalized.toarray())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3pQjgPeKvJ6A",
        "outputId": "0c1b91ba-8eaa-44aa-ad38-bb81a4594042"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['data' 'language' 'learning' 'love' 'loves' 'machine' 'natural'\n",
            " 'processing']\n",
            "[[0.         0.25       0.         0.25       0.         0.\n",
            "  0.25       0.25      ]\n",
            " [0.         0.         0.33333333 0.33333333 0.         0.33333333\n",
            "  0.         0.        ]\n",
            " [0.25       0.         0.25       0.         0.25       0.25\n",
            "  0.         0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF-IDF"
      ],
      "metadata": {
        "id": "_kh573dMvefL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf.fit_transform(documents)\n",
        "\n",
        "print(tfidf.get_feature_names_out())\n",
        "print(tfidf_matrix.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i94mAlpYvgpz",
        "outputId": "f695aaa1-cc96-42d2-cac4-222056094c9f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['data' 'language' 'learning' 'love' 'loves' 'machine' 'natural'\n",
            " 'processing']\n",
            "[[0.         0.52863461 0.         0.40204024 0.         0.\n",
            "  0.52863461 0.52863461]\n",
            " [0.         0.         0.57735027 0.57735027 0.         0.57735027\n",
            "  0.         0.        ]\n",
            " [0.5628291  0.         0.42804604 0.         0.5628291  0.42804604\n",
            "  0.         0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word2Vec"
      ],
      "metadata": {
        "id": "hysRRWPDv81N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "krIdc2JGyHML",
        "outputId": "f46262cd-5b21-4ac9-e074-e22d2190437d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
            "Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m40.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gensim\n",
            "Successfully installed gensim-4.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "tokenized_docs = [word_tokenize(doc.lower()) for doc in documents]\n",
        "\n",
        "w2v = Word2Vec(\n",
        "    sentences = tokenized_docs,\n",
        "    vector_size = 50,\n",
        "    window = 3,\n",
        "    min_count = 1,\n",
        "    workers = 2\n",
        ")\n",
        "\n",
        "print(w2v.wv['machine'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FsTivaQOv-oT",
        "outputId": "bbd0cfe9-2b77-4537-e450-d6313498e997"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.01631583  0.0089916  -0.00827415  0.00164907  0.01699724 -0.00892435\n",
            "  0.009035   -0.01357392 -0.00709698  0.01879702 -0.00315531  0.00064274\n",
            " -0.00828126 -0.01536538 -0.00301602  0.00493959 -0.00177605  0.01106732\n",
            " -0.00548595  0.00452013  0.01091159  0.01669191 -0.00290748 -0.01841629\n",
            "  0.0087411   0.00114357  0.01488382 -0.00162657 -0.00527683 -0.01750602\n",
            " -0.00171311  0.00565313  0.01080286  0.01410531 -0.01140624  0.00371764\n",
            "  0.01217773 -0.0095961  -0.00621452  0.01359526  0.00326295  0.00037983\n",
            "  0.00694727  0.00043555  0.01923765  0.01012121 -0.01783478 -0.01408312\n",
            "  0.00180291  0.01278507]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "word_tokenize(\"This is a test sentence.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZTP3PKWsyn0-",
        "outputId": "45ffea67-fa31-470f-d9fe-7829063f8d91"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['This', 'is', 'a', 'test', 'sentence', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    }
  ]
}