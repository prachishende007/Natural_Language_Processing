{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "P3vP9eBAzyXe"
      },
      "outputs": [],
      "source": [
        "# sample dataset\n",
        "\n",
        "texts = [\n",
        "    \"I loved the movie!\",\n",
        "    \"The movie was terrible\"\n",
        "]\n",
        "\n",
        "labels = [\"positive\", 'negative']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cleaning + Lemmatization"
      ],
      "metadata": {
        "id": "dJtim5r30aMH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def clean_text(text):\n",
        "  text = text.lower()\n",
        "  text = re.sub(r'[^a-z\\s]', '', text)\n",
        "  tokens = word_tokenize(text)\n",
        "  tokens = [lemmatizer.lemmatize(w) for w in tokens if w not in stop_words]\n",
        "  return \" \".join(tokens)\n",
        "\n",
        "cleaned_texts = [clean_text(t) for t in texts]\n",
        "print(cleaned_texts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wFRFkAqT0ioA",
        "outputId": "12366aea-0185-4c25-b876-0bd061c52020"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['loved movie', 'movie terrible']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Label Encoding"
      ],
      "metadata": {
        "id": "zp8ZKG6_1zsi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "le = LabelEncoder()\n",
        "encoded_labels = le.fit_transform(labels)\n",
        "print(encoded_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWITUZnJ12UW",
        "outputId": "108c8f6e-f703-4f90-f5bc-cc5d0a561965"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF-IDF Representation"
      ],
      "metadata": {
        "id": "pvf4QTte2GKo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "p7Ktt31X3ivF"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf = TfidfVectorizer()\n",
        "x = tfidf.fit_transform(cleaned_texts)\n",
        "\n",
        "print(tfidf.get_feature_names_out())\n",
        "print(x.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cH3U3ucP2JOl",
        "outputId": "332b9d78-fc7d-4929-a224-b1010fbc5963"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['loved' 'movie' 'terrible']\n",
            "[[0.81480247 0.57973867 0.        ]\n",
            " [0.         0.57973867 0.81480247]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save Outputs"
      ],
      "metadata": {
        "id": "PjwYRLbI2bvw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame(x.toarray(), columns = tfidf.get_feature_names_out())\n",
        "df[\"label\"] = encoded_labels\n",
        "\n",
        "df.to_csv(\"processed_text_data.csv\", index = False)"
      ],
      "metadata": {
        "id": "mqtOO-2U2a45"
      },
      "execution_count": 17,
      "outputs": []
    }
  ]
}